{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tutorial of pytorch by myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detahch\n",
    "\n",
    "PyTorch Detach creates a sensor where the storage is shared with another tensor with no grad involved, and thus a new tensor is returned which has no attachments with the current gradients. \n",
    "https://www.educba.com/pytorch-detach/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected derivative is $\\frac{\\partial i}{\\partial a}=4a^3+6a^5=224$. \n",
    "\n",
    "- If we detach the $c$ from the computational graph, all the backward error propagation trough the node of $c$ will disapear. Thus, $\\frac{\\partial i}{\\partial a}=4a^3=32$. \n",
    "- If we block all the nodes, the error back propogation will be stopped totally, thus no derivative  $\\frac{\\partial i}{\\partial a}$ can be calculate, there will be an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([224.])\n",
      "tensor([224.], grad_fn=<AddBackward0>)\n",
      "detach c from the computational graph\n",
      "tensor([32.])\n",
      "tensor([32.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([2.0], requires_grad=True)\n",
    "b=a**4\n",
    "c=a**6\n",
    "i=(b+c).sum()\n",
    "i.backward()\n",
    "print(a.grad)\n",
    "print(4*a**3+6*a**5)\n",
    "print('detach c from the computational graph')\n",
    "a=torch.tensor([2.0], requires_grad=True)\n",
    "b=a**4\n",
    "c=a**6\n",
    "c=c.detach()\n",
    "i=(b+c).sum()\n",
    "i.backward()\n",
    "print(a.grad)\n",
    "print(4*a**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block all the nodes b,c\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/user/Senior-Thesis/PyTorch Notes/my_pytorch1.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/user/Senior-Thesis/PyTorch%20Notes/my_pytorch1.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m c\u001b[39m=\u001b[39mc\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/user/Senior-Thesis/PyTorch%20Notes/my_pytorch1.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m i\u001b[39m=\u001b[39m(b\u001b[39m+\u001b[39mc)\u001b[39m.\u001b[39msum()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/user/Senior-Thesis/PyTorch%20Notes/my_pytorch1.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m i\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/user/Senior-Thesis/PyTorch%20Notes/my_pytorch1.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(a\u001b[39m.\u001b[39mgrad)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Senior-Thesis/PyTorch%20Notes/my_pytorch1.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m4\u001b[39m\u001b[39m*\u001b[39ma\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spytorch/lib/python3.10/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spytorch/lib/python3.10/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    155\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    156\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "print('block all the nodes b,c')\n",
    "a=torch.tensor([2.0], requires_grad=True)\n",
    "b=a**4\n",
    "c=a**6\n",
    "b=b.detach()\n",
    "c=c.detach()\n",
    "i=(b+c).sum()\n",
    "i.backward()\n",
    "print(a.grad)\n",
    "print(4*a**3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detach method create a tensor which share the same storage with the other. If we modify the detached tensor, this calculation can't be tracked. There will be error: \n",
    "- 'one of the variables needed for gradient computation has been modified by an inplace operation'\n",
    "\n",
    "Torch clone method will fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0.], requires_grad=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [5]] is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/user/Senior-Thesis/PyTorch Notes/my_pytorch1.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/user/Senior-Thesis/PyTorch%20Notes/my_pytorch1.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m o\u001b[39m.\u001b[39mzero_()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/user/Senior-Thesis/PyTorch%20Notes/my_pytorch1.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/user/Senior-Thesis/PyTorch%20Notes/my_pytorch1.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m n\u001b[39m.\u001b[39;49msum()\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/user/Senior-Thesis/PyTorch%20Notes/my_pytorch1.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(m\u001b[39m.\u001b[39mgrad)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spytorch/lib/python3.10/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/spytorch/lib/python3.10/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    155\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    156\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [5]] is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "m = torch.arange(5., requires_grad=True)\n",
    "n = m**2\n",
    "o = m.detach()\n",
    "o.zero_()\n",
    "print(m)\n",
    "n.sum().backward()\n",
    "print(m.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.arange(5., requires_grad=True)\n",
    "n = m**2\n",
    "o = m.detach().clone()\n",
    "o.zero_()\n",
    "print(m)\n",
    "n.sum().backward()\n",
    "print(m.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete or overwriting variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A non-leaf tensor doesn't need to be saved to perform the derivative, or error backpropagation. Deleting. or overwriting a non leaf tensor is fine  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete b \n",
      "tensor([192.])\n",
      "192\n"
     ]
    }
   ],
   "source": [
    "print('delete b ')\n",
    "a=torch.tensor([2.0], requires_grad=True)\n",
    "b=a**3\n",
    "c=b**2\n",
    "del b\n",
    "c.backward()\n",
    "print(a.grad)\n",
    "print(6*2**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overwrite b \n",
      "tensor([192.])\n",
      "192\n"
     ]
    }
   ],
   "source": [
    "print('overwrite b ')\n",
    "a=torch.tensor([2.0], requires_grad=True)\n",
    "b=a**3\n",
    "b=b**2\n",
    "b.backward()\n",
    "print(a.grad)\n",
    "print(6*2**5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't overwrite a leaft variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([192.])\n",
      "tensor([192.], grad_fn=<MulBackward0>)\n",
      "delete previous a by overwriting by itself\n",
      "tensor([8.], grad_fn=<PowBackward0>)\n",
      "None\n",
      "now a is treated as one non leaves tensor, there is no grad\n",
      "delete previous a by overwriting by itself, but with additional operation\n",
      "None\n",
      "now a is treated as one non leaves tensor, there is no grad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user/opt/anaconda3/envs/spytorch/lib/python3.10/site-packages/torch/_tensor.py:1013: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  /private/var/folders/sy/f16zz6x50xz3113nwtb9bvq00000gp/T/abs_croot-udngs7fm/pytorch_1648016055234/work/build/aten/src/ATen/core/TensorBody.h:417.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor([2.0], requires_grad=True)\n",
    "b=a**3\n",
    "c=b**2\n",
    "c.backward()\n",
    "print(a.grad)\n",
    "print(6*a**5)\n",
    "\n",
    "print('delete previous a by overwriting by itself')\n",
    "a=torch.tensor([2.0], requires_grad=True)\n",
    "a=a**3\n",
    "print(a)\n",
    "c=a**2\n",
    "c.backward()\n",
    "print(a.grad)\n",
    "print('now a is treated as one non leaves tensor, there is no grad')\n",
    "\n",
    "\n",
    "print('delete previous a by overwriting by itself, but with additional operation')\n",
    "a=torch.tensor([2.0], requires_grad=True)\n",
    "a=a.clone()**3\n",
    "# a.requires_grad=True\n",
    "c=a**2\n",
    "c.backward()\n",
    "print(a.grad)\n",
    "print('now a is treated as one non leaves tensor, there is no grad')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('block all the nodes b,c')\n",
    "a=torch.tensor([2.0], requires_grad=True)\n",
    "a=a**3\n",
    "print(a)\n",
    "c=a**2\n",
    "c.backward()\n",
    "print(a)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to modify leaf variable, but not tracking by autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([2.0], requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    a.pow_(3) # where add_ is an inplace operation\n",
    "b=0.5*a**2\n",
    "b.backward()\n",
    "print(a.grad)\n",
    "print('a**3 is not tracked by the autograd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([2.0], requires_grad=True)\n",
    "c=a**3\n",
    "b=0.5*c**2\n",
    "b.backward()\n",
    "print(a.grad)\n",
    "print(3*a**5)\n",
    "print('a**3 is tracked by the autograd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topic='mem'\n",
    "for i in range(1, len(net.layer_struct)):\n",
    "    # the neuron states\n",
    "    neuron_states=getattr(net, 'hist_{}{}_rec'.format(plot_topic, i))[epoch_index][sampling_index][example_index][time_range_time_step[0]:time_range_time_step[1],:]\n",
    "\n",
    "    # load the post spk\n",
    "    output=getattr(net, 'hist_spk{}_rec'.format(i))[epoch_index][sampling_index][:,time_range_time_step[0]:time_range_time_step[1],:]\n",
    "    output_list=tensor_to_spike_lists(output,net.time_step)\n",
    "\n",
    "    # load the pre spk\n",
    "    if i-1==0:\n",
    "        input=getattr(net, 'spk{}_rec'.format(i-1))[sampling_index][:,time_range_time_step[0]:time_range_time_step[1],:]\n",
    "        print(input.size())\n",
    "        input_list=tensor_to_spike_lists(input,net.time_step)\n",
    "    else:\n",
    "        input=getattr(net, 'hist_spk{}_rec'.format(i-1))[epoch_index][sampling_index][:,time_range_time_step[0]:time_range_time_step[1],:]\n",
    "        input_list=tensor_to_spike_lists(input,net.time_step)\n",
    "    \n",
    "    fig, axs = plt.subplots(net.layer_struct[i], sharex=True, sharey=True)\n",
    "    x=np.arange(0, net.duration, net.time_step)[time_range_time_step[0]:time_range_time_step[1]]\n",
    "    if net.layer_struct[i]>1:\n",
    "        for neuron_index in range(net.layer_struct[i]):\n",
    "            # plot the neuron state\n",
    "            axs[neuron_index].plot(x, neuron_states[:,neuron_index])\n",
    "            # plot the post spk\n",
    "            axs[neuron_index].eventplot(output_list[example_index][neuron_index], lineoffsets=0.5, linelengths=1, color='red',linestyles='--')\n",
    "\n",
    "            # plot the pre spk\n",
    "            for neuron_index_pre in range(net.layer_struct[i-1]):\n",
    "                w=getattr(net, 'hist_w{}_rec'.format(i))[epoch_index][sampling_index][neuron_index_pre, neuron_index]\n",
    "                axs[neuron_index].eventplot(input_list[example_index][neuron_index_pre], lineoffsets=0.5*w, linelengths=w, color='green',linestyles='-.')\n",
    "    else:\n",
    "      axs.plot(x, neuron_states[:,0])\n",
    "      axs.eventplot(output_list[example_index][0], lineoffsets=0.5, linelengths=1, color='red', linestyles='--')\n",
    "      for neuron_index_pre in range(net.layer_struct[i-1]):\n",
    "          w=getattr(net, 'hist_w{}_rec'.format(i))[epoch_index][sampling_index][neuron_index_pre,0]\n",
    "          axs.eventplot(input_list[example_index][neuron_index_pre], lineoffsets=0.5*w, linelengths=w, color='green',linestyles='-.')\n",
    "\n",
    "\n",
    "\n",
    "    fig.supxlabel('time (ms)')\n",
    "    fig.supylabel('{}'.format(plot_topic))\n",
    "    fig.suptitle('{} of layer {}'.format(plot_topic,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('spytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f51df8c237cfff43f594d2cbd3826b4705755914f532f86790fe777fc6074838"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
