{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tutorial of pytorch by myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/klein/anaconda3/envs/SG/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save all overwrited tensor in a list\n",
    "\n",
    "The overwrited or deleted tensor are still be saved in the graph of pytorch, error back propogation can be performed still. But because we lost their name in the python namespace, it's hard to retrieve thoese names' grad. One solution is to save all tensor in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad for a\n",
      "tensor([960.,  30.])\n",
      "grad for b1\n",
      "tensor([80., 10.])\n",
      "grad for b2\n",
      "tensor([5., 5.])\n"
     ]
    }
   ],
   "source": [
    "# the original scenario\n",
    "a=torch.tensor([2.0, 1.0], requires_grad=True)\n",
    "b1=a**3\n",
    "b2=b1**2\n",
    "b1.retain_grad()\n",
    "b2.retain_grad()\n",
    "c=torch.sum(5*b2)\n",
    "c.backward()\n",
    "print('grad for a')\n",
    "print(a.grad)\n",
    "print('grad for b1')\n",
    "print(b1.grad)\n",
    "print('grad for b2')\n",
    "print(b2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad for a\n",
      "tensor([960.,  30.])\n",
      "grad for b\n",
      "tensor([5., 5.])\n",
      "only the last b's grad is accessible\n"
     ]
    }
   ],
   "source": [
    "# overwrite b1 iwth b2\n",
    "a=torch.tensor([2.0, 1.0], requires_grad=True)\n",
    "b=a**3\n",
    "b=b**2\n",
    "b.retain_grad()\n",
    "c=torch.sum(5*b)\n",
    "c.backward()\n",
    "print('grad for a')\n",
    "print(a.grad)\n",
    "print('grad for b')\n",
    "print(b.grad)\n",
    "print('only the last b\\'s grad is accessible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad for a\n",
      "tensor([960.,  30.])\n",
      "grad for b1\n",
      "tensor([80., 10.])\n",
      "grad for b2\n",
      "tensor([5., 5.])\n"
     ]
    }
   ],
   "source": [
    "# save b and overwrited b in a list\n",
    "a=torch.tensor([2.0, 1.0], requires_grad=True)\n",
    "b_list=[]\n",
    "b=a**3\n",
    "b.retain_grad()\n",
    "b_list.append(b)\n",
    "b=b**2 # it seems the memory of the previous b tensor is saved in the graph, not deleted or overwrited\n",
    "b.retain_grad()\n",
    "b_list.append(b)\n",
    "c=torch.sum(5*b)\n",
    "c.backward()\n",
    "print('grad for a')\n",
    "print(a.grad)\n",
    "print('grad for b1')\n",
    "print(b_list[0].grad)\n",
    "print('grad for b2')\n",
    "print(b_list[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking a list to tensor won't block the error bp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad for a1\n",
      "tensor([960.,  30.])\n",
      "grad for a2\n",
      "tensor([ 30., 960.])\n",
      "grad for b_tensor\n",
      "tensor([[80., 10.],\n",
      "        [10., 80.]])\n"
     ]
    }
   ],
   "source": [
    "b_list=[]\n",
    "a1=torch.tensor([2.0, 1.0], requires_grad=True)\n",
    "b=a1**3\n",
    "b.retain_grad()\n",
    "b_list.append(b)\n",
    "a2=torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "b=a2**3\n",
    "b.retain_grad()\n",
    "b_list.append(b)\n",
    "\n",
    "b_tensor=torch.stack(b_list)\n",
    "b_tensor.retain_grad()\n",
    "\n",
    "d=b_tensor**2\n",
    "d.retain_grad()\n",
    "\n",
    "c=torch.sum(5*d)\n",
    "c.backward()\n",
    "\n",
    "print('grad for a1')\n",
    "print(a1.grad)\n",
    "\n",
    "print('grad for a2')\n",
    "print(a2.grad)\n",
    "\n",
    "print('grad for b_tensor')\n",
    "print(b_tensor.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inplace operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An in-place operation is an operation that changes directly the content of a given Tensor without making a copy. Inplace operations in pytorch are always postfixed with a _, like .add_() or .scatter_(). Python operations like += or *= are also inplace operations.\n",
    "\n",
    "In-place operations save some memory, but can be problematic when computing derivatives because of an immediate loss of history. Hence, their use is discouraged\n",
    "\n",
    "Changing elements of a tensor is also a inplace operation. But it seems fine for autograd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
