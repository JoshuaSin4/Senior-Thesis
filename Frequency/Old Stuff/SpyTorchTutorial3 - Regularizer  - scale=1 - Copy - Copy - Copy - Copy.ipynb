{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For more details on surrogate gradient learning, please see: \n",
    "> Neftci, E.O., Mostafa, H., and Zenke, F. (2019). Surrogate Gradient Learning in Spiking Neural Networks.\n",
    "> https://arxiv.org/abs/1901.09948"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3: Training a spiking neural network on a simple vision dataset\n",
    "\n",
    "Friedemann Zenke (https://fzenke.net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Tutorial 2, we have seen how to train a simple multi-layer spiking neural network on the [Fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist). However, the spiking activity in the hidden layer was not particularly plausible in a biological sense. Here we modify the network from this previous tutorial by adding activity regularizer, which encourages solutions with sparse spiking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coarse network structure is dicated by the Fashion MNIST dataset. \n",
    "nb_inputs  = 28*28\n",
    "nb_hidden  = 100\n",
    "nb_outputs = 10\n",
    "\n",
    "time_step = 1e-2\n",
    "nb_steps  = 100\n",
    "\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "\n",
    "# Check whether a GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")     \n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load the Dataset\n",
    "root = os.path.expanduser(\"~/data/datasets/torch/mnist\")\n",
    "train_dataset = torchvision.datasets.MNIST(root, train=True, transform=None, target_transform=None, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root, train=False, transform=None, target_transform=None, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joshua\\AppData\\Local\\Temp\\ipykernel_20064\\1224777063.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x_train = np.array(train_dataset.data, dtype=np.float)\n",
      "C:\\Users\\Joshua\\AppData\\Local\\Temp\\ipykernel_20064\\1224777063.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x_test = np.array(test_dataset.data, dtype=np.float)\n"
     ]
    }
   ],
   "source": [
    "# Standardize data\n",
    "# x_train = torch.tensor(train_dataset.train_data, device=device, dtype=dtype)\n",
    "x_train = np.array(train_dataset.data, dtype=np.float)\n",
    "x_train = x_train.reshape(x_train.shape[0],-1)/255\n",
    "# x_test = torch.tensor(test_dataset.test_data, device=device, dtype=dtype)\n",
    "x_test = np.array(test_dataset.data, dtype=np.float)\n",
    "x_test = x_test.reshape(x_test.shape[0],-1)/255\n",
    "\n",
    "# y_train = torch.tensor(train_dataset.train_labels, device=device, dtype=dtype)\n",
    "# y_test  = torch.tensor(test_dataset.test_labels, device=device, dtype=dtype)\n",
    "y_train = np.array(train_dataset.targets, dtype=np.int64)\n",
    "y_test  = np.array(test_dataset.targets, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 27.5, 27.5, -0.5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we plot one of the raw data points as an example\n",
    "data_id = 2\n",
    "plt.imshow(x_train[data_id].reshape(28,28), cmap=plt.cm.gray_r)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with spiking neural networks, we ideally want to use a temporal code to make use of spike timing. To that end, we will use a spike latency code to feed spikes to our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current2firing_time(x, tau=20, thr=0.3, tmax=1.0, epsilon=1e-7):\n",
    "    \"\"\" Computes first firing time latency for a current input x assuming the charge time of a current based LIF neuron.\n",
    "\n",
    "    Args:\n",
    "    x -- The \"current\" values\n",
    "\n",
    "    Keyword args:\n",
    "    tau -- The membrane time constant of the LIF neuron to be charged\n",
    "    thr -- The firing threshold value \n",
    "    tmax -- The maximum time returned \n",
    "    epsilon -- A generic (small) epsilon > 0\n",
    "\n",
    "    Returns:\n",
    "    Time to first spike for each \"current\" x\n",
    "    \"\"\"\n",
    "    idx = x<thr\n",
    "    x = np.clip(x,thr+epsilon,1e9)\n",
    "    T = tau*np.log(x/(x-thr))\n",
    "    T[idx] = tmax\n",
    "    return T\n",
    "\n",
    "def spike_fn(x):\n",
    "    out = torch.zeros_like(x)\n",
    "    out[x > 0] = 1.0\n",
    "    return out\n",
    "        \n",
    "def sparse_data_generator(X, y, batch_size, nb_steps, nb_units, shuffle=True ):\n",
    "    \"\"\" This generator takes datasets in analog format and generates spiking network input as sparse tensors. \n",
    "\n",
    "    Args:\n",
    "        X: The data ( sample x event x 2 ) the last dim holds (time,neuron) tuples\n",
    "        y: The labels\n",
    "    \"\"\"\n",
    "\n",
    "    labels_ = np.array(y,dtype=np.int64)\n",
    "    number_of_batches = len(X)//batch_size\n",
    "    sample_index = np.arange(len(X))\n",
    "\n",
    "    # compute discrete firing times\n",
    "    tau_eff = 20e-3/time_step\n",
    "    firing_times = np.array(current2firing_time(X, tau=tau_eff, tmax=nb_steps), dtype=np.int64)\n",
    "    unit_numbers = np.arange(nb_units)\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "\n",
    "    total_batch_count = 0\n",
    "    counter = 0\n",
    "    while counter<number_of_batches:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "\n",
    "        coo = [ [] for i in range(3) ]\n",
    "        for bc,idx in enumerate(batch_index):\n",
    "            \n",
    "            batch = [bc for _ in range(len(times))]\n",
    "            coo[0].extend(batch)\n",
    "            \n",
    "\n",
    "        i = torch.LongTensor(coo).to(device)\n",
    "        v = torch.FloatTensor(np.ones(len(coo[0]))).to(device)\n",
    "    \n",
    "        X_batch = torch.sparse.FloatTensor(i, v, torch.Size([batch_size,nb_steps,nb_units])).to(device)\n",
    "        y_batch = torch.tensor(labels_[batch_index],device=device)\n",
    "\n",
    "        yield X_batch.to(device=device), y_batch.to(device=device)\n",
    "\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_image(image, scale, time_step):\n",
    "    scaled_image = image*scale\n",
    "    for i,prob in enumerate(scaled_image):\n",
    "        if (prob > 1):\n",
    "            new_prob = 1\n",
    "            scaled_image[i] = new_prob\n",
    "    rate_of_scaled_image = scaled_image/time_step\n",
    "    average_rate = torch.mean(rate_of_scaled_image)\n",
    "    return scaled_image, average_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images2spike(x, y, batch_size, shuffle, **kwargs):  \n",
    "    '''Converts images to spike trains'''\n",
    "    labels_ = np.array(y,dtype=np.int64)\n",
    "    number_of_batches = len(x)//batch_size\n",
    "    sample_index = np.arange(len(x))\n",
    "\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "\n",
    "    total_batch_count = 0\n",
    "    counter = 0\n",
    "\n",
    "    batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "    while counter < number_of_batches:\n",
    "        x_batch = torch.empty((len(x[batch_index]), nb_steps, nb_inputs)).to(device)\n",
    "        for i, image in enumerate(x[batch_index]):\n",
    "            tensor_image = torch.Tensor(image) # probabilities tensor\n",
    "            zero_image = torch.zeros(tensor_image.shape)\n",
    "            spike_train = torch.empty((nb_steps, nb_inputs))\n",
    "            for t in range(nb_steps):\n",
    "                spike_t = torch.bernoulli(tensor_image)\n",
    "                spike_train[t] = spike_t\n",
    "            x_batch[i] = spike_train\n",
    "        y_batch = torch.tensor(labels_[batch_index],device=device) \n",
    "\n",
    "        yield x_batch,  y_batch\n",
    "\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of the spiking network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_mem = 10e-3\n",
    "tau_syn = 5e-3\n",
    "\n",
    "alpha   = float(np.exp(-time_step/tau_syn))\n",
    "beta    = float(np.exp(-time_step/tau_mem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init done\n"
     ]
    }
   ],
   "source": [
    "weight_scale = 0.002\n",
    "\n",
    "w1 = torch.empty((nb_inputs, nb_hidden),  device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(w1, mean=0.0, std=weight_scale)\n",
    "\n",
    "w2 = torch.empty((nb_hidden, nb_outputs), device=device, dtype=dtype, requires_grad=True)\n",
    "torch.nn.init.normal_(w2, mean=0.0, std=weight_scale)\n",
    "\n",
    "print(\"init done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_voltage_traces(mem, spk=None, dim=(3,5), spike_height=5):\n",
    "    gs=GridSpec(*dim)\n",
    "    if spk is not None:\n",
    "        dat = 1.0*mem\n",
    "        dat[spk>0.0] = spike_height\n",
    "        dat = dat.detach().cpu().numpy()\n",
    "    else:\n",
    "        dat = mem.detach().cpu().numpy()\n",
    "    for i in range(np.prod(dim)):\n",
    "        if i==0: a0=ax=plt.subplot(gs[i])\n",
    "        else: ax=plt.subplot(gs[i],sharey=a0)\n",
    "        ax.plot(dat[i])\n",
    "        ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here we implement our spiking nonlinearity which also implements \n",
    "    the surrogate gradient. By subclassing torch.autograd.Function, \n",
    "    we will be able to use all of PyTorch's autograd functionality.\n",
    "    Here we use the normalized negative part of a fast sigmoid \n",
    "    as this was done in Zenke & Ganguli (2018).\n",
    "    \"\"\"\n",
    "    \n",
    "    scale = 100.0 # controls steepness of surrogate gradient\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which \n",
    "        we need to later backpropagate our error signals. To achieve this we use the \n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor we need to compute the \n",
    "        surrogate gradient of the loss with respect to the input. \n",
    "        Here we use the normalized negative part of a fast sigmoid \n",
    "        as this was done in Zenke & Ganguli (2018).\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
    "        return grad\n",
    "    \n",
    "# here we overwrite our naive spike function by the \"SurrGradSpike\" nonlinearity which implements a surrogate gradient\n",
    "spike_fn  = SurrGradSpike.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_snn(inputs):\n",
    "    h1 = torch.einsum(\"abc,cd->abd\", (inputs, w1))\n",
    "    syn = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
    "    mem = torch.zeros((batch_size,nb_hidden), device=device, dtype=dtype)\n",
    "\n",
    "    mem_rec = []\n",
    "    spk_rec = []\n",
    "\n",
    "    # Compute hidden layer activity\n",
    "    for t in range(nb_steps):\n",
    "        mthr = mem-1.0\n",
    "        out = spike_fn(mthr)\n",
    "        rst = out.detach() # We do not want to backprop through the reset\n",
    "\n",
    "        new_syn = alpha*syn +h1[:,t]\n",
    "        new_mem = (beta*mem +syn)*(1.0-rst)\n",
    "\n",
    "        mem_rec.append(mem)\n",
    "        spk_rec.append(out)\n",
    "        \n",
    "        mem = new_mem\n",
    "        syn = new_syn\n",
    "\n",
    "    mem_rec = torch.stack(mem_rec,dim=1)\n",
    "    spk_rec = torch.stack(spk_rec,dim=1)\n",
    "\n",
    "    # Readout layer\n",
    "    h2= torch.einsum(\"abc,cd->abd\", (spk_rec, w2))\n",
    "    flt = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
    "    out = torch.zeros((batch_size,nb_outputs), device=device, dtype=dtype)\n",
    "    out_rec = [out]\n",
    "    spk_rec2 = []\n",
    "    for t in range(nb_steps):\n",
    "        mthr = out-1.0\n",
    "        output = spike_fn(mthr)\n",
    "        rst = output.detach() # We do not want to backprop through the reset\n",
    "\n",
    "        new_flt = alpha*flt +h2[:,t]\n",
    "        new_out = (beta*out +flt)*(1.0-rst)\n",
    "\n",
    "        flt = new_flt\n",
    "        out = new_out\n",
    "\n",
    "        out_rec.append(out) # membrane potential\n",
    "        spk_rec2.append(output) # spike train\n",
    "\n",
    "    out_rec = torch.stack(out_rec,dim=1)\n",
    "    spk_rec2 = torch.stack(spk_rec2, dim=1)\n",
    "\n",
    "    return spk_rec2,  spk_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_data, y_data, scale=1, lr=1e-3, nb_epochs=10):\n",
    "    params = [w1,w2]\n",
    "    optimizer = torch.optim.Adamax(params, lr=lr, betas=(0.9,0.999))\n",
    "\n",
    "    log_softmax_fn = nn.LogSoftmax(dim=1)\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    \n",
    "    loss_hist = []\n",
    "    for e in range(nb_epochs):\n",
    "        local_loss = []\n",
    "        for x_local, y_local in images2spike(x_data, y_data, batch_size, shuffle=True):\n",
    "            output, spks = run_snn(x_local)\n",
    "            spike_count =torch.sum(output,1)\n",
    "            mean_firing_rate = spike_count/(nb_steps*time_step)\n",
    "\n",
    "            log_p_y = log_softmax_fn(mean_firing_rate)\n",
    "            \n",
    "            # Here we set up our regularizer loss\n",
    "            # The strength paramters here are merely a guess and there should be ample room for improvement by\n",
    "            # tuning these paramters.\n",
    "            reg_loss = 1e-6*torch.sum(spks) # L1 loss on total number of spikes\n",
    "            reg_loss += 1e-7*torch.mean(torch.sum(torch.sum(spks,dim=0),dim=0)**2) # L2 loss on spikes per neuron\n",
    "\n",
    "            # Here we combine supervised loss and the regularizer\n",
    "            loss_val = loss_fn(log_p_y, y_local) \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            local_loss.append(loss_val.item())\n",
    "        mean_loss = np.mean(local_loss)\n",
    "        print(\"Epoch %i: loss=%.5f\"%(e+1,mean_loss))\n",
    "        loss_hist.append(mean_loss)\n",
    "        \n",
    "    return loss_hist\n",
    "        \n",
    "        \n",
    "def compute_classification_accuracy(x_data, y_data, batch_size, shuffle, **kwargs):\n",
    "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
    "    accs = []\n",
    "    for x_local, y_local in images2spike(x_data, y_data, batch_size, shuffle=True, **kwargs):\n",
    "        output, _ = run_snn(x_local)\n",
    "        spike_count =torch.sum(output,1)\n",
    "        mean_firing_rate = spike_count/(nb_steps*time_step)\n",
    "        _, am = torch.max(mean_firing_rate, 1)\n",
    "        tmp = np.mean((y_local==am).detach().cpu().numpy()) # compare to labels\n",
    "        accs.append(tmp)\n",
    "    return np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=2.30258\n",
      "Epoch 2: loss=2.13347\n",
      "Epoch 3: loss=1.50847\n",
      "Epoch 4: loss=0.80739\n",
      "Epoch 5: loss=0.64385\n",
      "Epoch 6: loss=0.44844\n",
      "Epoch 7: loss=0.36450\n",
      "Epoch 8: loss=0.31247\n",
      "Epoch 9: loss=0.21224\n",
      "Epoch 10: loss=0.33104\n",
      "Epoch 11: loss=0.30271\n",
      "Epoch 12: loss=0.23508\n",
      "Epoch 13: loss=0.14311\n",
      "Epoch 14: loss=0.31555\n",
      "Epoch 15: loss=0.22731\n",
      "Epoch 16: loss=0.22082\n",
      "Epoch 17: loss=0.23696\n",
      "Epoch 18: loss=0.11230\n",
      "Epoch 19: loss=0.13154\n",
      "Epoch 20: loss=0.14493\n",
      "Epoch 21: loss=0.17053\n",
      "Epoch 22: loss=0.19044\n",
      "Epoch 23: loss=0.16080\n",
      "Epoch 24: loss=0.14962\n",
      "Epoch 25: loss=0.09289\n",
      "Epoch 26: loss=0.17284\n",
      "Epoch 27: loss=0.17543\n",
      "Epoch 28: loss=0.21032\n",
      "Epoch 29: loss=0.14746\n",
      "Epoch 30: loss=0.20464\n",
      "Epoch 31: loss=0.17430\n",
      "Epoch 32: loss=0.12366\n",
      "Epoch 33: loss=0.18202\n",
      "Epoch 34: loss=0.08410\n",
      "Epoch 35: loss=0.13481\n",
      "Epoch 36: loss=0.05808\n",
      "Epoch 37: loss=0.13251\n",
      "Epoch 38: loss=0.20386\n",
      "Epoch 39: loss=0.09169\n",
      "Epoch 40: loss=0.18236\n",
      "Epoch 41: loss=0.04225\n",
      "Epoch 42: loss=0.11236\n",
      "Epoch 43: loss=0.13247\n",
      "Epoch 44: loss=0.11675\n",
      "Epoch 45: loss=0.13760\n",
      "Epoch 46: loss=0.05697\n",
      "Epoch 47: loss=0.13940\n",
      "Epoch 48: loss=0.06493\n",
      "Epoch 49: loss=0.05571\n",
      "Epoch 50: loss=0.10007\n",
      "Epoch 51: loss=0.09843\n",
      "Epoch 52: loss=0.07146\n",
      "Epoch 53: loss=0.03544\n",
      "Epoch 54: loss=0.09722\n",
      "Epoch 55: loss=0.05384\n",
      "Epoch 56: loss=0.09478\n",
      "Epoch 57: loss=0.11724\n",
      "Epoch 58: loss=0.06819\n",
      "Epoch 59: loss=0.07698\n",
      "Epoch 60: loss=0.07565\n",
      "Epoch 61: loss=0.05555\n",
      "Epoch 62: loss=0.07633\n",
      "Epoch 63: loss=0.05807\n",
      "Epoch 64: loss=0.07867\n",
      "Epoch 65: loss=0.06710\n",
      "Epoch 66: loss=0.07708\n",
      "Epoch 67: loss=0.03748\n",
      "Epoch 68: loss=0.04849\n",
      "Epoch 69: loss=0.05417\n",
      "Epoch 70: loss=0.08080\n",
      "Epoch 71: loss=0.09705\n",
      "Epoch 72: loss=0.14024\n",
      "Epoch 73: loss=0.05173\n",
      "Epoch 74: loss=0.03648\n",
      "Epoch 75: loss=0.06897\n",
      "Epoch 76: loss=0.10870\n",
      "Epoch 77: loss=0.12017\n",
      "Epoch 78: loss=0.07301\n",
      "Epoch 79: loss=0.08057\n",
      "Epoch 80: loss=0.15790\n",
      "Epoch 81: loss=0.10954\n",
      "Epoch 82: loss=0.11419\n",
      "Epoch 83: loss=0.09164\n",
      "Epoch 84: loss=0.12582\n",
      "Epoch 85: loss=0.09514\n",
      "Epoch 86: loss=0.07298\n",
      "Epoch 87: loss=0.07654\n",
      "Epoch 88: loss=0.06745\n",
      "Epoch 89: loss=0.06537\n",
      "Epoch 90: loss=0.12172\n",
      "Epoch 91: loss=0.09728\n",
      "Epoch 92: loss=0.04001\n",
      "Epoch 93: loss=0.03375\n",
      "Epoch 94: loss=0.03948\n",
      "Epoch 95: loss=0.08102\n",
      "Epoch 96: loss=0.03693\n",
      "Epoch 97: loss=0.08043\n",
      "Epoch 98: loss=0.04663\n",
      "Epoch 99: loss=0.10870\n",
      "Epoch 100: loss=0.07656\n",
      "Epoch 101: loss=0.08744\n",
      "Epoch 102: loss=0.05593\n",
      "Epoch 103: loss=0.03908\n",
      "Epoch 104: loss=0.09488\n",
      "Epoch 105: loss=0.03640\n",
      "Epoch 106: loss=0.02735\n",
      "Epoch 107: loss=0.04686\n",
      "Epoch 108: loss=0.04170\n",
      "Epoch 109: loss=0.02716\n",
      "Epoch 110: loss=0.02082\n",
      "Epoch 111: loss=0.03750\n",
      "Epoch 112: loss=0.06008\n",
      "Epoch 113: loss=0.04366\n",
      "Epoch 114: loss=0.06704\n",
      "Epoch 115: loss=0.04072\n",
      "Epoch 116: loss=0.04218\n",
      "Epoch 117: loss=0.05020\n",
      "Epoch 118: loss=0.07660\n",
      "Epoch 119: loss=0.03241\n",
      "Epoch 120: loss=0.02397\n",
      "Epoch 121: loss=0.02622\n",
      "Epoch 122: loss=0.04144\n",
      "Epoch 123: loss=0.06508\n",
      "Epoch 124: loss=0.05563\n",
      "Epoch 125: loss=0.04459\n",
      "Epoch 126: loss=0.01372\n",
      "Epoch 127: loss=0.05041\n",
      "Epoch 128: loss=0.06304\n",
      "Epoch 129: loss=0.02921\n",
      "Epoch 130: loss=0.03348\n",
      "Epoch 131: loss=0.05970\n",
      "Epoch 132: loss=0.04903\n",
      "Epoch 133: loss=0.01538\n",
      "Epoch 134: loss=0.04068\n",
      "Epoch 135: loss=0.03908\n",
      "Epoch 136: loss=0.03636\n",
      "Epoch 137: loss=0.04009\n",
      "Epoch 138: loss=0.01725\n",
      "Epoch 139: loss=0.10554\n",
      "Epoch 140: loss=0.00610\n",
      "Epoch 141: loss=0.07675\n",
      "Epoch 142: loss=0.02439\n",
      "Epoch 143: loss=0.06044\n",
      "Epoch 144: loss=0.11831\n",
      "Epoch 145: loss=0.04822\n",
      "Epoch 146: loss=0.06726\n",
      "Epoch 147: loss=0.04895\n",
      "Epoch 148: loss=0.06628\n",
      "Epoch 149: loss=0.03892\n",
      "Epoch 150: loss=0.05226\n",
      "Epoch 151: loss=0.01654\n",
      "Epoch 152: loss=0.03399\n",
      "Epoch 153: loss=0.08103\n",
      "Epoch 154: loss=0.05868\n",
      "Epoch 155: loss=0.08065\n",
      "Epoch 156: loss=0.03503\n",
      "Epoch 157: loss=0.01442\n",
      "Epoch 158: loss=0.07964\n",
      "Epoch 159: loss=0.05615\n",
      "Epoch 160: loss=0.07297\n",
      "Epoch 161: loss=0.06169\n",
      "Epoch 162: loss=0.06254\n",
      "Epoch 163: loss=0.03628\n",
      "Epoch 164: loss=0.03433\n",
      "Epoch 165: loss=0.04862\n",
      "Epoch 166: loss=0.04096\n",
      "Epoch 167: loss=0.01622\n",
      "Epoch 168: loss=0.05064\n",
      "Epoch 169: loss=0.05451\n",
      "Epoch 170: loss=0.03823\n",
      "Epoch 171: loss=0.04199\n",
      "Epoch 172: loss=0.06355\n",
      "Epoch 173: loss=0.02365\n",
      "Epoch 174: loss=0.03444\n",
      "Epoch 175: loss=0.02234\n",
      "Epoch 176: loss=0.02871\n",
      "Epoch 177: loss=0.03828\n",
      "Epoch 178: loss=0.07277\n",
      "Epoch 179: loss=0.01013\n",
      "Epoch 180: loss=0.06861\n",
      "Epoch 181: loss=0.01967\n",
      "Epoch 182: loss=0.09743\n",
      "Epoch 183: loss=0.04385\n",
      "Epoch 184: loss=0.07507\n",
      "Epoch 185: loss=0.02187\n",
      "Epoch 186: loss=0.03162\n",
      "Epoch 187: loss=0.04912\n",
      "Epoch 188: loss=0.04924\n",
      "Epoch 189: loss=0.04279\n",
      "Epoch 190: loss=0.00971\n",
      "Epoch 191: loss=0.02204\n",
      "Epoch 192: loss=0.04867\n",
      "Epoch 193: loss=0.01182\n",
      "Epoch 194: loss=0.01887\n",
      "Epoch 195: loss=0.02754\n",
      "Epoch 196: loss=0.04652\n",
      "Epoch 197: loss=0.07708\n",
      "Epoch 198: loss=0.01965\n",
      "Epoch 199: loss=0.02419\n",
      "Epoch 200: loss=0.04619\n",
      "Epoch 201: loss=0.02230\n",
      "Epoch 202: loss=0.02736\n",
      "Epoch 203: loss=0.05325\n",
      "Epoch 204: loss=0.05960\n",
      "Epoch 205: loss=0.06533\n",
      "Epoch 206: loss=0.02459\n",
      "Epoch 207: loss=0.02563\n",
      "Epoch 208: loss=0.01811\n",
      "Epoch 209: loss=0.01847\n",
      "Epoch 210: loss=0.01520\n",
      "Epoch 211: loss=0.04880\n",
      "Epoch 212: loss=0.02728\n",
      "Epoch 213: loss=0.02714\n",
      "Epoch 214: loss=0.04531\n",
      "Epoch 215: loss=0.07212\n",
      "Epoch 216: loss=0.02548\n",
      "Epoch 217: loss=0.01784\n",
      "Epoch 218: loss=0.01870\n",
      "Epoch 219: loss=0.01775\n",
      "Epoch 220: loss=0.00727\n",
      "Epoch 221: loss=0.02645\n",
      "Epoch 222: loss=0.03546\n",
      "Epoch 223: loss=0.04737\n",
      "Epoch 224: loss=0.02625\n",
      "Epoch 225: loss=0.01849\n",
      "Epoch 226: loss=0.01629\n",
      "Epoch 227: loss=0.02945\n",
      "Epoch 228: loss=0.03456\n",
      "Epoch 229: loss=0.03837\n",
      "Epoch 230: loss=0.02888\n",
      "Epoch 231: loss=0.04296\n",
      "Epoch 232: loss=0.02430\n",
      "Epoch 233: loss=0.01089\n",
      "Epoch 234: loss=0.01698\n",
      "Epoch 235: loss=0.02538\n",
      "Epoch 236: loss=0.01633\n",
      "Epoch 237: loss=0.02381\n",
      "Epoch 238: loss=0.02473\n",
      "Epoch 239: loss=0.02818\n",
      "Epoch 240: loss=0.06040\n",
      "Epoch 241: loss=0.02516\n",
      "Epoch 242: loss=0.00653\n",
      "Epoch 243: loss=0.02060\n",
      "Epoch 244: loss=0.02186\n",
      "Epoch 245: loss=0.02700\n",
      "Epoch 246: loss=0.06713\n",
      "Epoch 247: loss=0.03642\n",
      "Epoch 248: loss=0.02894\n",
      "Epoch 249: loss=0.03237\n",
      "Epoch 250: loss=0.01920\n",
      "Epoch 251: loss=0.02461\n",
      "Epoch 252: loss=0.01842\n",
      "Epoch 253: loss=0.02539\n",
      "Epoch 254: loss=0.01722\n",
      "Epoch 255: loss=0.03031\n",
      "Epoch 256: loss=0.02782\n",
      "Epoch 257: loss=0.04119\n",
      "Epoch 258: loss=0.01124\n",
      "Epoch 259: loss=0.02761\n",
      "Epoch 260: loss=0.02584\n",
      "Epoch 261: loss=0.02426\n",
      "Epoch 262: loss=0.02554\n",
      "Epoch 263: loss=0.04814\n",
      "Epoch 264: loss=0.01539\n",
      "Epoch 265: loss=0.05280\n",
      "Epoch 266: loss=0.04425\n",
      "Epoch 267: loss=0.01219\n",
      "Epoch 268: loss=0.03330\n",
      "Epoch 269: loss=0.01207\n",
      "Epoch 270: loss=0.01702\n",
      "Epoch 271: loss=0.01597\n",
      "Epoch 272: loss=0.06087\n",
      "Epoch 273: loss=0.02872\n",
      "Epoch 274: loss=0.03417\n",
      "Epoch 275: loss=0.04466\n",
      "Epoch 276: loss=0.01666\n",
      "Epoch 277: loss=0.01681\n",
      "Epoch 278: loss=0.03373\n",
      "Epoch 279: loss=0.02973\n",
      "Epoch 280: loss=0.02982\n",
      "Epoch 281: loss=0.00766\n",
      "Epoch 282: loss=0.00865\n",
      "Epoch 283: loss=0.01974\n",
      "Epoch 284: loss=0.02293\n",
      "Epoch 285: loss=0.00664\n",
      "Epoch 286: loss=0.01502\n",
      "Epoch 287: loss=0.03481\n",
      "Epoch 288: loss=0.08142\n",
      "Epoch 289: loss=0.02043\n",
      "Epoch 290: loss=0.03167\n",
      "Epoch 291: loss=0.01515\n",
      "Epoch 292: loss=0.01184\n",
      "Epoch 293: loss=0.01564\n",
      "Epoch 294: loss=0.00670\n",
      "Epoch 295: loss=0.01168\n",
      "Epoch 296: loss=0.04339\n",
      "Epoch 297: loss=0.02360\n",
      "Epoch 298: loss=0.04225\n",
      "Epoch 299: loss=0.04970\n",
      "Epoch 300: loss=0.04309\n",
      "Epoch 301: loss=0.02064\n",
      "Epoch 302: loss=0.01294\n",
      "Epoch 303: loss=0.01352\n",
      "Epoch 304: loss=0.04646\n",
      "Epoch 305: loss=0.01218\n",
      "Epoch 306: loss=0.01951\n",
      "Epoch 307: loss=0.05463\n",
      "Epoch 308: loss=0.02381\n",
      "Epoch 309: loss=0.01606\n",
      "Epoch 310: loss=0.00947\n",
      "Epoch 311: loss=0.02060\n",
      "Epoch 312: loss=0.02204\n",
      "Epoch 313: loss=0.01815\n",
      "Epoch 314: loss=0.03106\n",
      "Epoch 315: loss=0.02608\n",
      "Epoch 316: loss=0.01663\n",
      "Epoch 317: loss=0.01267\n",
      "Epoch 318: loss=0.02160\n",
      "Epoch 319: loss=0.04748\n",
      "Epoch 320: loss=0.02046\n",
      "Epoch 321: loss=0.04280\n",
      "Epoch 322: loss=0.02254\n",
      "Epoch 323: loss=0.03412\n",
      "Epoch 324: loss=0.02018\n",
      "Epoch 325: loss=0.00901\n",
      "Epoch 326: loss=0.04453\n",
      "Epoch 327: loss=0.03904\n",
      "Epoch 328: loss=0.02365\n",
      "Epoch 329: loss=0.01713\n",
      "Epoch 330: loss=0.01183\n",
      "Epoch 331: loss=0.00576\n",
      "Epoch 332: loss=0.01680\n",
      "Epoch 333: loss=0.02486\n",
      "Epoch 334: loss=0.05253\n",
      "Epoch 335: loss=0.02344\n",
      "Epoch 336: loss=0.01449\n",
      "Epoch 337: loss=0.01291\n",
      "Epoch 338: loss=0.02428\n",
      "Epoch 339: loss=0.01674\n",
      "Epoch 340: loss=0.00457\n",
      "Epoch 341: loss=0.00344\n",
      "Epoch 342: loss=0.01200\n",
      "Epoch 343: loss=0.01191\n",
      "Epoch 344: loss=0.01337\n",
      "Epoch 345: loss=0.02029\n",
      "Epoch 346: loss=0.03418\n",
      "Epoch 347: loss=0.03632\n",
      "Epoch 348: loss=0.01449\n",
      "Epoch 349: loss=0.01934\n",
      "Epoch 350: loss=0.00683\n",
      "Epoch 351: loss=0.01226\n",
      "Epoch 352: loss=0.03109\n",
      "Epoch 353: loss=0.06306\n",
      "Epoch 354: loss=0.02708\n",
      "Epoch 355: loss=0.01340\n",
      "Epoch 356: loss=0.04284\n",
      "Epoch 357: loss=0.04349\n",
      "Epoch 358: loss=0.01932\n",
      "Epoch 359: loss=0.01504\n",
      "Epoch 360: loss=0.03049\n",
      "Epoch 361: loss=0.02733\n",
      "Epoch 362: loss=0.04260\n",
      "Epoch 363: loss=0.02480\n",
      "Epoch 364: loss=0.03530\n",
      "Epoch 365: loss=0.02280\n",
      "Epoch 366: loss=0.01863\n",
      "Epoch 367: loss=0.00680\n",
      "Epoch 368: loss=0.01552\n",
      "Epoch 369: loss=0.01589\n",
      "Epoch 370: loss=0.01790\n",
      "Epoch 371: loss=0.02097\n",
      "Epoch 372: loss=0.02046\n",
      "Epoch 373: loss=0.01659\n",
      "Epoch 374: loss=0.01834\n",
      "Epoch 375: loss=0.01531\n",
      "Epoch 376: loss=0.01274\n",
      "Epoch 377: loss=0.02491\n",
      "Epoch 378: loss=0.01091\n",
      "Epoch 379: loss=0.04222\n",
      "Epoch 380: loss=0.01734\n",
      "Epoch 381: loss=0.02081\n",
      "Epoch 382: loss=0.02242\n",
      "Epoch 383: loss=0.01350\n",
      "Epoch 384: loss=0.04578\n",
      "Epoch 385: loss=0.06888\n",
      "Epoch 386: loss=0.01783\n",
      "Epoch 387: loss=0.01401\n",
      "Epoch 388: loss=0.01489\n",
      "Epoch 389: loss=0.00963\n",
      "Epoch 390: loss=0.01688\n",
      "Epoch 391: loss=0.02191\n",
      "Epoch 392: loss=0.00426\n",
      "Epoch 393: loss=0.01767\n",
      "Epoch 394: loss=0.02873\n",
      "Epoch 395: loss=0.02272\n",
      "Epoch 396: loss=0.03359\n",
      "Epoch 397: loss=0.01037\n",
      "Epoch 398: loss=0.00936\n",
      "Epoch 399: loss=0.01233\n",
      "Epoch 400: loss=0.01185\n"
     ]
    }
   ],
   "source": [
    "loss_hist = train(x_train, y_train, lr=2e-4, nb_epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 495x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(3.3,2),dpi=150)\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.953\n",
      "Test accuracy: 0.964\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy: %.3f\"%(compute_classification_accuracy(x_train,y_train, batch_size=256, shuffle=True)))\n",
    "print(\"Test accuracy: %.3f\"%(compute_classification_accuracy(x_test,y_test, batch_size=256, shuffle=True)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Oct 24 2022, 16:02:16) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "59c89e211e6a7d15bcf1fda4d2617053683adc75be6d99dab9047beccd836cf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
